{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "id": "37bd244e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "node_embeddings_list = []\n",
    "edge_embeddings_list = []\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "f6a71059-b2f7-4a48-b7ac-686a66ce5bc4",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Subject: http://example.org/A2, Predicate: http://example.org/connected_to, Object: http://example.org/C2\n",
      "Subject: http://example.org/A1, Predicate: http://example.org/connected_to, Object: http://example.org/C1\n",
      "Subject: http://example.org/A3, Predicate: http://example.org/connected_to, Object: http://example.org/C3\n",
      "Subject: http://example.org/A1, Predicate: http://example.org/related_to, Object: http://example.org/B1\n",
      "Subject: http://example.org/A4, Predicate: http://example.org/related_to, Object: http://example.org/B4\n",
      "Subject: http://example.org/B1, Predicate: http://example.org/connected_to, Object: http://example.org/C1\n",
      "Subject: http://example.org/B3, Predicate: http://example.org/connected_to, Object: http://example.org/C3\n",
      "Subject: http://example.org/A5, Predicate: http://example.org/related_to, Object: http://example.org/B5\n",
      "Subject: http://example.org/A3, Predicate: http://example.org/related_to, Object: http://example.org/B3\n",
      "Subject: http://example.org/B2, Predicate: http://example.org/connected_to, Object: http://example.org/C2\n",
      "Subject: http://example.org/A2, Predicate: http://example.org/related_to, Object: http://example.org/B2\n",
      "Graph serialized and saved to ./data/rdf_graph.nt.\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "c:\\Python312\\Lib\\site-packages\\rdflib\\plugins\\serializers\\nt.py:41: UserWarning: NTSerializer always uses UTF-8 encoding. Given encoding was: None\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "from rdflib import Graph, Literal, RDF, RDFS, URIRef, Namespace\n",
    "import random\n",
    "from pathlib import Path\n",
    "import os\n",
    "\n",
    "# Define the namespace again\n",
    "ex = Namespace(\"http://example.org/\")\n",
    "\n",
    "# Create a new RDF graph\n",
    "g = Graph()\n",
    "file_path = Path(\"data/rdf_graph.nt\")\n",
    "\n",
    "# if file_path.exists():\n",
    "#     # Read the graph from the existing file if it exists\n",
    "#     g.parse(file_path, format=\"nt\")\n",
    "#     print(f\"Graph loaded from {file_path}.\")\n",
    "# else:\n",
    "    # Define classes\n",
    "g.add((ex.A, RDF.type, RDFS.Class))\n",
    "g.add((ex.B, RDF.type, RDFS.Class))\n",
    "g.add((ex.C, RDF.type, RDFS.Class))\n",
    "\n",
    "# Define possible colors\n",
    "colors = [\"red\", \"blue\", \"green\", \"yellow\", \"purple\", \"orange\"]\n",
    "\n",
    "# Helper function to generate nodes with instance number, name, and color\n",
    "def create_node(g, class_type, node_id, color):\n",
    "    node_uri = URIRef(f\"{class_type}{node_id}\")\n",
    "    g.add((node_uri, RDF.type, class_type))\n",
    "    g.add((node_uri, ex.instance_number, Literal(node_id)))\n",
    "    g.add((node_uri, ex.name, Literal(f\"{class_type}{node_id}\")))\n",
    "    g.add((node_uri, ex.color, Literal(color)))\n",
    "    return node_uri\n",
    "\n",
    "# Create 5 nodes for class A, B, and C with instance number, name, and color\n",
    "for i in range(1, 6):\n",
    "    color_A = colors[i % len(colors)]  # Assign color based on modulo to reuse colors\n",
    "    color_B = colors[(i + 1) % len(colors)]\n",
    "    color_C = colors[(i + 2) % len(colors)]\n",
    "    \n",
    "    node_B = create_node(g, ex.A, i, color_A)  # Now A is being used for node B\n",
    "    node_A = create_node(g, ex.B, i, color_B)  # Now B is being used for node A\n",
    "    node_C = create_node(g, ex.C, i, color_C)  # C remains the same\n",
    "\n",
    "    # First 3 nodes of A are connected to C nodes (positive classification)\n",
    "    if i <= 3:\n",
    "        g.add((node_A, ex.connected_to, node_C))\n",
    "        g.add((node_B, ex.connected_to, node_C))\n",
    "    else:\n",
    "        # For nodes A4 and A5, no connections to C nodes (negative classification)\n",
    "        # Ensure no connection to C nodes (negative classification)\n",
    "        pass\n",
    "\n",
    "# Add extra relations to increase complexity\n",
    "for i in range(1, 6):\n",
    "    node_A = URIRef(f\"http://example.org/A{i}\")\n",
    "    node_B = URIRef(f\"http://example.org/B{i}\")\n",
    "    g.add((node_A, ex.related_to, node_B))\n",
    "\n",
    "\n",
    "def classify_node_A(graph, node_A):\n",
    "    \"\"\"\n",
    "    Determines if a node of type A is connected to any node of type C.\n",
    "    If A has an outgoing edge to any C node, it's classified as 'positive',\n",
    "    otherwise, it's 'negative'.\n",
    "    \"\"\"\n",
    "    connected_to_C = False\n",
    "    \n",
    "    # Iterate over all triples where node_A is the subject and 'connected_to' is the predicate\n",
    "    for _, _, obj in graph.triples((node_A, ex.connected_to, None)):  # Ensure A -> C\n",
    "        if obj.startswith(f\"{ex}C\"):  # Ensure the target is a C node\n",
    "            connected_to_C = True\n",
    "            break  # No need to check further if a connection exists\n",
    "    \n",
    "    return \"positive\" if connected_to_C else \"negative\"\n",
    "\n",
    "        \n",
    "# Add classification triples for node A\n",
    "for i in range(1, 6):\n",
    "    node_A = URIRef(f\"http://example.org/A{i}\")\n",
    "    classification = classify_node_A(g, node_A)\n",
    "    g.add((node_A, ex.classification, Literal(classification)))\n",
    "\n",
    "# Save the graph to a file in NT format (triples)\n",
    "graph_file_path = \"./data/rdf_graph.nt\"\n",
    "\n",
    "# Create the directory if it doesn't exist\n",
    "os.makedirs(os.path.dirname(graph_file_path), exist_ok=True)\n",
    "\n",
    "# Print all edges (triples) where predicate is 'connected_to' or 'related_to'\n",
    "for subj, pred, obj in g:\n",
    "    if str(pred) in [\"http://example.org/connected_to\", \"http://example.org/related_to\"]:\n",
    "        print(f\"Subject: {subj}, Predicate: {pred}, Object: {obj}\")\n",
    "\n",
    "\n",
    "# Serialize and write the graph to the file properly\n",
    "g.serialize(destination=graph_file_path, format=\"nt\")\n",
    "print(f\"Graph serialized and saved to {graph_file_path}.\")\n",
    "\n",
    "\n",
    "\n",
    "        \n",
    "    # # Print the graph in NT format (triples)\n",
    "    # print(g.serialize(format=\"nt\"))\n",
    "\n",
    "\n",
    "# Print all edges (triples) in the graph\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "da6dc024-7b21-40f4-b93a-0b2f2fd58ddc",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Added 3 edge indices for 'A -> C'.\n",
      "Added 3 edge indices for 'B -> C'.\n",
      "Added 5 edge indices for 'A -> B'.\n",
      "HeteroData(\n",
      "  A={\n",
      "    x=[5, 3],\n",
      "    y=[5],\n",
      "    train_mask=[5],\n",
      "    test_mask=[5],\n",
      "  },\n",
      "  B={ x=[5, 3] },\n",
      "  C={ x=[5, 3] },\n",
      "  (A, connected_to, C)={\n",
      "    edge_index=[2, 3],\n",
      "    edge_type=[3],\n",
      "  },\n",
      "  (B, connected_to, C)={\n",
      "    edge_index=[2, 3],\n",
      "    edge_type=[3],\n",
      "  },\n",
      "  (A, related_to, B)={\n",
      "    edge_index=[2, 5],\n",
      "    edge_type=[5],\n",
      "  }\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch_geometric.data import HeteroData\n",
    "from rdflib import Graph, Namespace, RDF, URIRef, Literal\n",
    "from rdflib.namespace import RDFS\n",
    "import random\n",
    "\n",
    "# Define the RDF namespace\n",
    "ex = Namespace(\"http://example.org/\")\n",
    "\n",
    "# Initialize the HeteroData object\n",
    "hetero_data = HeteroData()\n",
    "\n",
    "# Create dictionaries to map node URIs to indices\n",
    "node_mapping = {\n",
    "    'A': {},\n",
    "    'B': {},\n",
    "    'C': {}\n",
    "}\n",
    "\n",
    "# Function to safely extract literal values\n",
    "def get_literal_value(graph, subject, predicate, default_value=None):\n",
    "    value = graph.value(subject, predicate)\n",
    "    return value if value is not None else default_value\n",
    "\n",
    "# Function to extract node features from the RDF graph\n",
    "def extract_node_features(graph, class_type, node_type):\n",
    "    node_features = []\n",
    "    index = 0\n",
    "    for s, p, o in graph.triples((None, RDF.type, class_type)):\n",
    "        instance_number = get_literal_value(graph, s, ex.instance_number, 0)\n",
    "        name = str(get_literal_value(graph, s, ex.name, \"Unknown\"))\n",
    "        color = str(get_literal_value(graph, s, ex.color, \"black\"))\n",
    "\n",
    "        node_id = s.split(\"/\")[-1]\n",
    "        node_mapping[node_type][node_id] = index\n",
    "\n",
    "        instance_tensor = torch.tensor([float(instance_number)], dtype=torch.float32)\n",
    "        color_tensor = torch.tensor([hash(color) % 1000], dtype=torch.float32)\n",
    "        name_tensor = torch.tensor([hash(name) % 1000], dtype=torch.float32)\n",
    "\n",
    "        node_features.append(torch.cat([instance_tensor, color_tensor, name_tensor], dim=0))\n",
    "        index += 1\n",
    "\n",
    "    if node_features:\n",
    "        hetero_data[node_type].x = torch.stack(node_features)\n",
    "\n",
    "# Extract node features for classes A, B, and C\n",
    "extract_node_features(g, ex.A, 'A')\n",
    "extract_node_features(g, ex.B, 'B')\n",
    "extract_node_features(g, ex.C, 'C')\n",
    "\n",
    "# Edge extraction function ensuring correct mapping\n",
    "def extract_edges(graph, source_class, target_class, relation):\n",
    "    edge_list = []\n",
    "    for s, p, o in graph.triples((None, relation, None)):\n",
    "        source_id = s.split(\"/\")[-1]\n",
    "        target_id = o.split(\"/\")[-1]\n",
    "        \n",
    "        if source_id in node_mapping[source_class] and target_id in node_mapping[target_class]:\n",
    "            source_idx = node_mapping[source_class][source_id]\n",
    "            target_idx = node_mapping[target_class][target_id]\n",
    "            edge_list.append((source_idx, target_idx))\n",
    "    \n",
    "    return edge_list\n",
    "\n",
    "# Extract 'connected_to' edges (A -> C, B -> C)\n",
    "connected_to_edges_a_c = extract_edges(g, \"A\", \"C\", ex.connected_to)\n",
    "connected_to_edges_b_c = extract_edges(g, \"B\", \"C\", ex.connected_to)\n",
    "\n",
    "# Extract 'related_to' edges (A -> B)\n",
    "related_to_edges_a_b = extract_edges(g, \"A\", \"B\", ex.related_to)\n",
    "\n",
    "# Add edge indices and edge types to HeteroData\n",
    "if connected_to_edges_a_c:\n",
    "    hetero_data['A', 'connected_to', 'C'].edge_index = torch.tensor(connected_to_edges_a_c, dtype=torch.long).t().contiguous()\n",
    "    hetero_data['A', 'connected_to', 'C'].edge_type = torch.zeros(len(connected_to_edges_a_c), dtype=torch.long)\n",
    "    print(f\"Added {len(connected_to_edges_a_c)} edge indices for 'A -> C'.\")\n",
    "\n",
    "if connected_to_edges_b_c:\n",
    "    hetero_data['B', 'connected_to', 'C'].edge_index = torch.tensor(connected_to_edges_b_c, dtype=torch.long).t().contiguous()\n",
    "    hetero_data['B', 'connected_to', 'C'].edge_type = torch.ones(len(connected_to_edges_b_c), dtype=torch.long)\n",
    "    print(f\"Added {len(connected_to_edges_b_c)} edge indices for 'B -> C'.\")\n",
    "\n",
    "if related_to_edges_a_b:\n",
    "    hetero_data['A', 'related_to', 'B'].edge_index = torch.tensor(related_to_edges_a_b, dtype=torch.long).t().contiguous()\n",
    "    hetero_data['A', 'related_to', 'B'].edge_type = torch.full((len(related_to_edges_a_b),), 2, dtype=torch.long)\n",
    "    print(f\"Added {len(related_to_edges_a_b)} edge indices for 'A -> B'.\")\n",
    "\n",
    "# Create labels for node type A based on RDF graph classification\n",
    "labels_A = []\n",
    "for s, p, o in g.triples((None, ex.classification, None)):\n",
    "    if str(s).startswith(\"http://example.org/A\"):\n",
    "        label = 1 if str(o) == \"positive\" else 0\n",
    "        labels_A.append(label)\n",
    "\n",
    "if labels_A:\n",
    "    hetero_data['A'].y = torch.tensor(labels_A, dtype=torch.long)\n",
    "\n",
    "# Create train and test masks for node type 'A'\n",
    "num_nodes_A = len(hetero_data['A'].y)\n",
    "indices_A = list(range(num_nodes_A))\n",
    "random.shuffle(indices_A)\n",
    "\n",
    "train_size_A = int(0.8 * num_nodes_A)\n",
    "train_indices_A = indices_A[:train_size_A]\n",
    "test_indices_A = indices_A[train_size_A:]\n",
    "\n",
    "train_mask_A = torch.zeros(num_nodes_A, dtype=torch.bool)\n",
    "train_mask_A[train_indices_A] = True\n",
    "\n",
    "test_mask_A = torch.zeros(num_nodes_A, dtype=torch.bool)\n",
    "test_mask_A[test_indices_A] = True\n",
    "\n",
    "hetero_data['A'].train_mask = train_mask_A\n",
    "hetero_data['A'].test_mask = test_mask_A\n",
    "\n",
    "# Ensure edges exist before extracting them\n",
    "if ('A', 'connected_to', 'C') in hetero_data and ('B', 'connected_to', 'C') in hetero_data:\n",
    "    edge_index_A_to_C = hetero_data[('A', 'connected_to', 'C')].edge_index\n",
    "    edge_index_B_to_C = hetero_data[('B', 'connected_to', 'C')].edge_index\n",
    "\n",
    "    edge_index = torch.cat([edge_index_A_to_C, edge_index_B_to_C], dim=1)\n",
    "\n",
    "    edge_type_A_to_C = hetero_data[('A', 'connected_to', 'C')].edge_type\n",
    "    edge_type_B_to_C = hetero_data[('B', 'connected_to', 'C')].edge_type\n",
    "\n",
    "    edge_type = torch.cat([edge_type_A_to_C, edge_type_B_to_C], dim=0)\n",
    "    print(f\"Final combined edge index shape: {edge_index.shape}\")\n",
    "\n",
    "print(hetero_data)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "263439b2-71d2-48ef-b00c-90ff5613aad0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Node features for A: torch.Size([5, 3])\n",
      "Node features for B: torch.Size([5, 3])\n",
      "Node features for C: torch.Size([5, 3])\n",
      "Labels for A: torch.Size([5])\n",
      "Edge index from A to C: tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n",
      "Edge index from B to C: tensor([[0, 1, 2],\n",
      "        [0, 1, 2]])\n",
      "Edge index from A to B: tensor([[0, 1, 2, 3, 4],\n",
      "        [0, 1, 2, 3, 4]])\n"
     ]
    }
   ],
   "source": [
    "# Assuming 'data' is your HeteroData object\n",
    "# Accessing node features\n",
    "x_A = hetero_data['A'].x  # Node features for type A\n",
    "x_B = hetero_data['B'].x  # Node features for type B\n",
    "x_C = hetero_data['C'].x  # Node features for type C\n",
    "y_A = hetero_data['A'].y  # Labels for type C nodes\n",
    "\n",
    "# Accessing edge indices for different relationships\n",
    "edge_index_A_to_C = hetero_data[('A', 'connected_to', 'C')].edge_index  # Edge index from C to A\n",
    "edge_index_B_to_C = hetero_data[('B', 'connected_to', 'C')].edge_index  # Edge index from C to B\n",
    "edge_index_A_to_B = hetero_data[('A', 'related_to', 'B')].edge_index  # Edge index from A to B\n",
    "\n",
    "# Example: Printing the extracted data\n",
    "print(\"Node features for A:\", x_A.shape)\n",
    "print(\"Node features for B:\", x_B.shape)\n",
    "print(\"Node features for C:\", x_C.shape)\n",
    "print(\"Labels for A:\", y_A.shape)\n",
    "\n",
    "print(\"Edge index from A to C:\", edge_index_A_to_C)\n",
    "print(\"Edge index from B to C:\", edge_index_B_to_C)\n",
    "print(\"Edge index from A to B:\", edge_index_A_to_B)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6f697a55-320d-4370-81f0-676a919a65e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Loaded GNN model from ./saved_models/A_gnn.pth\n",
      "GNN model saved to ./saved_models/A_gnn.pth\n",
      "[0, 1, 2]\n",
      "[3, 4]\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# import torch\n",
    "# import torch.nn.functional as F\n",
    "# from torch_geometric.nn import FastRGCNConv\n",
    "\n",
    "# # Define the FastRGCN-based model\n",
    "# class FastRGCNGNN(torch.nn.Module):\n",
    "#     def __init__(self, num_relations):\n",
    "#         super().__init__()\n",
    "#         self.conv1 = FastRGCNConv(in_channels=3, out_channels=32, num_relations=num_relations)\n",
    "#         self.conv2 = FastRGCNConv(in_channels=32, out_channels=64, num_relations=num_relations)\n",
    "#         self.lin = torch.nn.Linear(64, 2)  # Binary classification (positive or negative)\n",
    "\n",
    "#     def forward(self, x, edge_index, edge_type=None):\n",
    "#         x = F.relu(self.conv1(x, edge_index, edge_type))  # Pass node features x to the first layer\n",
    "#         x = self.conv2(x, edge_index, edge_type)\n",
    "#         return self.lin(x)  # Apply the final linear layer\n",
    "\n",
    "# # Initialize the model\n",
    "# num_relations = 2  # Adjust according to your relations\n",
    "# model = FastRGCNGNN(num_relations)\n",
    "\n",
    "# # Define the optimizer and loss function\n",
    "# optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "# criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "# def train(hetero_data):\n",
    "#     model.train()  # Set the model to training mode\n",
    "#     optimizer.zero_grad()  # Clear gradients\n",
    "\n",
    "#     # Extracting node features and edge indices for node type A\n",
    "#     x_A = hetero_data['A'].x  # Node features for A\n",
    "\n",
    "#     # Ensure correct shape for x_A\n",
    "#     if len(x_A.shape) != 2 or x_A.shape[1] != 3:  # Should be [num_nodes, num_features]\n",
    "#         print(f\"Unexpected shape for x_A: {x_A.shape}\")\n",
    "\n",
    "#     # Forward pass for node type A\n",
    "#     out = model(x_A, edge_index, edge_type)  # Pass edge_type to the model\n",
    "\n",
    "#     # Compute loss using masks for node type A\n",
    "#     loss = criterion(out[hetero_data['A'].train_mask], hetero_data['A'].y[hetero_data['A'].train_mask])\n",
    "\n",
    "#     # Backward pass: Compute gradients and update weights\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Calculate accuracy for node type A\n",
    "#     preds = out.argmax(dim=1)  # Get predicted class labels\n",
    "#     correct = (preds[hetero_data['A'].train_mask] == hetero_data['A'].y[hetero_data['A'].train_mask]).sum().item()  # Count correct predictions\n",
    "#     total_samples = hetero_data['A'].train_mask.sum().item()  # Count total samples in training\n",
    "\n",
    "#     accuracy = correct / total_samples if total_samples > 0 else 0  # Prevent division by zero\n",
    "\n",
    "#     return loss.item(), accuracy  # Return loss and accuracy\n",
    "\n",
    "\n",
    "# # Train the model for a number of epochs\n",
    "# for epoch in range(1, 20):  # Train for 20 epochs\n",
    "#     loss, accuracy = train(hetero_data)\n",
    "#     print(f'Epoch {epoch}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import FastRGCNConv\n",
    "\n",
    "# Define the FastRGCN-based model\n",
    "class FastRGCNGNN(torch.nn.Module):\n",
    "    def __init__(self, num_relations, in_channels=3, hidden_dim=32, out_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = FastRGCNConv(in_channels, hidden_dim, num_relations=num_relations)\n",
    "        self.conv2 = FastRGCNConv(hidden_dim, out_dim, num_relations=num_relations)\n",
    "        self.lin = torch.nn.Linear(out_dim, 2)  # Binary classification (positive or negative)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type=None):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_type))\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return self.lin(x)\n",
    "\n",
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.nn import FastRGCNConv\n",
    "\n",
    "# Define the FastRGCN-based model\n",
    "class FastRGCNGNN(torch.nn.Module):\n",
    "    def __init__(self, num_relations, in_channels=3, hidden_dim=32, out_dim=64):\n",
    "        super().__init__()\n",
    "        self.conv1 = FastRGCNConv(in_channels, hidden_dim, num_relations=num_relations)\n",
    "        self.conv2 = FastRGCNConv(hidden_dim, out_dim, num_relations=num_relations)\n",
    "        self.lin = torch.nn.Linear(out_dim, 2)  # Binary classification (positive or negative)\n",
    "\n",
    "    def forward(self, x, edge_index, edge_type=None):\n",
    "        x = F.relu(self.conv1(x, edge_index, edge_type))\n",
    "        x = self.conv2(x, edge_index, edge_type)\n",
    "        return self.lin(x)\n",
    "\n",
    "# Define the GNN Trainer Class\n",
    "class GNNTrainer:\n",
    "    def __init__(self, hetero_data, node_type, in_channels=3, hidden_dim=32, out_dim=64):\n",
    "        self.hetero_data = hetero_data\n",
    "        self.node_type = node_type\n",
    "\n",
    "        # ✅ Define model path\n",
    "        self.model_dir = \"./saved_models\"\n",
    "        self.model_path = f\"{self.model_dir}/{node_type}_gnn.pth\"\n",
    "\n",
    "        # ✅ Ensure directory exists\n",
    "        os.makedirs(self.model_dir, exist_ok=True)\n",
    "\n",
    "        self.model = FastRGCNGNN(num_relations=len(hetero_data.edge_types), in_channels=in_channels, hidden_dim=hidden_dim, out_dim=out_dim)\n",
    "        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=0.01)\n",
    "        self.criterion = torch.nn.CrossEntropyLoss()\n",
    "\n",
    "    def save_model(self):\n",
    "        \"\"\"Saves the trained model to a file.\"\"\"\n",
    "        torch.save(self.model.state_dict(), self.model_path)\n",
    "        print(f\"GNN model saved to {self.model_path}\")\n",
    "\n",
    "    def load_model(self):\n",
    "        \"\"\"Loads the trained model if it exists.\"\"\"\n",
    "        if os.path.exists(self.model_path):\n",
    "            self.model.load_state_dict(torch.load(self.model_path, weights_only=True))\n",
    "            print(f\" Loaded GNN model from {self.model_path}\")\n",
    "\n",
    "    def train(self, epochs=20):\n",
    "        \"\"\"Train the GNN model and save it.\"\"\"\n",
    "        if os.path.exists(self.model_path):\n",
    "            self.load_model()\n",
    "            return self.model\n",
    "\n",
    "        for epoch in range(epochs):\n",
    "            self.model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            x = self.hetero_data[self.node_type].x\n",
    "            edge_index = self.hetero_data[('A', 'connected_to', 'C')].edge_index\n",
    "            edge_type = self.hetero_data[('A', 'connected_to', 'C')].edge_type\n",
    "            out = self.model(x, edge_index, edge_type)\n",
    "            loss = self.criterion(out[self.hetero_data[self.node_type].train_mask], self.hetero_data[self.node_type].y[self.hetero_data[self.node_type].train_mask])\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "            preds = out.argmax(dim=1)\n",
    "            accuracy = (preds[self.hetero_data[self.node_type].train_mask] == self.hetero_data[self.node_type].y[self.hetero_data[self.node_type].train_mask]).float().mean().item()\n",
    "            print(f'Epoch {epoch+1}/{epochs}, Loss: {loss:.4f}, Accuracy: {accuracy:.4f}')\n",
    "\n",
    "        return self.model\n",
    "\n",
    "    \n",
    "    def get_positive_nodes(self):\n",
    "        \"\"\"Returns the indices of nodes classified as positive by the trained GNN model.\"\"\"\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get features & edges\n",
    "            x = self.hetero_data[self.node_type].x\n",
    "            edge_index = self.hetero_data[('A', 'connected_to', 'C')].edge_index\n",
    "            edge_type = self.hetero_data[('A', 'connected_to', 'C')].edge_type\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            out = self.model(x, edge_index, edge_type)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(out, dim=1)\n",
    "\n",
    "            # Classify nodes: 1 = Positive, 0 = Negative\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            # Filter only positively classified nodes\n",
    "            pos_nodes = torch.where(preds == 1)[0].tolist()\n",
    "\n",
    "            # print(f\"Predicted Class Labels: {preds.tolist()}\")  # Debugging print\n",
    "            # print(f\"Positive Nodes: {pos_nodes}\")  # Debugging print\n",
    "\n",
    "            return pos_nodes\n",
    "\n",
    "    def get_negative_nodes(self):\n",
    "        \"\"\"Returns the indices of nodes classified as negative by the trained GNN model.\"\"\"\n",
    "        self.model.eval()  # Ensure the model is in evaluation mode\n",
    "\n",
    "        with torch.no_grad():\n",
    "            # Get features & edges\n",
    "            x = self.hetero_data[self.node_type].x\n",
    "            edge_index = self.hetero_data[('A', 'connected_to', 'C')].edge_index\n",
    "            edge_type = self.hetero_data[('A', 'connected_to', 'C')].edge_type\n",
    "            \n",
    "            # Forward pass through the model\n",
    "            out = self.model(x, edge_index, edge_type)\n",
    "\n",
    "            # Apply softmax to get probabilities\n",
    "            probs = torch.softmax(out, dim=1)\n",
    "\n",
    "            # Classify nodes: 1 = Positive, 0 = Negative\n",
    "            preds = torch.argmax(probs, dim=1)\n",
    "\n",
    "            # Filter only negatively classified nodes\n",
    "            neg_nodes = torch.where(preds == 0)[0].tolist()\n",
    "\n",
    "            return neg_nodes\n",
    "    \n",
    "gnn = GNNTrainer(hetero_data,'A')\n",
    "gnn.train()\n",
    "gnn.save_model()\n",
    "print(gnn.get_positive_nodes())\n",
    "print(gnn.get_negative_nodes())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "1f0451d3-4b39-446f-8c49-1e3b81ac143c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TransE embeddings from file.\n"
     ]
    }
   ],
   "source": [
    "# import os\n",
    "# import pandas as pd\n",
    "# import torch\n",
    "# import torch.nn as nn\n",
    "# import torch.optim as optim\n",
    "# import numpy as np\n",
    "\n",
    "# # Define data directory\n",
    "# data_dir = \"./data\"\n",
    "# os.makedirs(data_dir, exist_ok=True)  # Ensure the directory exists\n",
    "\n",
    "# unique_nodes = set()\n",
    "# node_mapping = {}  # Dictionary to map (node type, local index) -> global index\n",
    "# global_index = 0   # Start from global index 0\n",
    "\n",
    "# for node_type in hetero_data.node_types:\n",
    "#     num_nodes = hetero_data[node_type].x.shape[0] if 'x' in hetero_data[node_type] else 0\n",
    "#     for local_idx in range(num_nodes):\n",
    "#         node_mapping[(node_type, local_idx)] = global_index  # Assign unique index\n",
    "#         unique_nodes.add(global_index)\n",
    "#         global_index += 1  # Increment global index\n",
    "\n",
    "# unique_nodes = sorted(list(unique_nodes))  # Ensure sorted list for indexing\n",
    "\n",
    "# # Create a node index mapping\n",
    "# node_to_index = {node: i for i, node in enumerate(unique_nodes)}\n",
    "\n",
    "# # Extract unique edges from HeteroData before using them\n",
    "# unique_edges = []\n",
    "# for edge_type in hetero_data.edge_types:\n",
    "#     edge_index = hetero_data[edge_type].edge_index\n",
    "#     for i in range(edge_index.shape[1]):  # Iterate over edges\n",
    "#         src = edge_index[0, i].item()  # Source node\n",
    "#         tgt = edge_index[1, i].item()  # Target node\n",
    "#         unique_edges.append((src, edge_type, tgt))  # Store edge as tuple (src, relation, tgt)\n",
    "\n",
    "\n",
    "# # Create mappings for edges and relations\n",
    "# relation_to_index = {rel: i for i, rel in enumerate(hetero_data.edge_types)}\n",
    "\n",
    "# # Convert edges to tensors\n",
    "# train_edges = torch.tensor([(node_to_index[h], relation_to_index[r], node_to_index[t]) for (h, r, t) in unique_edges])\n",
    "\n",
    "# # Define the TransE model again\n",
    "# class TransE(nn.Module):\n",
    "#     def __init__(self, num_entities, num_relations, embedding_dim):\n",
    "#         super(TransE, self).__init__()\n",
    "#         self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "#         self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "#         nn.init.xavier_uniform_(self.entity_embeddings.weight)\n",
    "#         nn.init.xavier_uniform_(self.relation_embeddings.weight)\n",
    "\n",
    "#     def forward(self, head, relation):\n",
    "#         head_emb = self.entity_embeddings(head)\n",
    "#         relation_emb = self.relation_embeddings(relation)\n",
    "#         return head_emb + relation_emb\n",
    "\n",
    "#     def score(self, head, relation, tail):\n",
    "#         \"\"\" Compute TransE score (L2 norm of (h + r - t)) \"\"\"\n",
    "#         h_r = self.forward(head, relation)\n",
    "#         t_emb = self.entity_embeddings(tail)\n",
    "#         return -torch.norm(h_r - t_emb, p=2, dim=1)  # Negative distance (higher = better)\n",
    "\n",
    "# # Initialize model\n",
    "# num_entities = len(unique_nodes)\n",
    "# num_relations = len(hetero_data.edge_types)\n",
    "# embedding_dim = 128\n",
    "\n",
    "# transe_model = TransE(num_entities, num_relations, embedding_dim)\n",
    "# optimizer = optim.Adam(transe_model.parameters(), lr=0.01)\n",
    "\n",
    "# # Define training loop\n",
    "# num_epochs = 100\n",
    "# batch_size = 32\n",
    "# loss_fn = nn.MarginRankingLoss(margin=1.0)\n",
    "\n",
    "# # Convert edges to tensors\n",
    "# train_edges = torch.tensor([(node_to_index[h], relation_to_index[r], node_to_index[t]) for (h, r, t) in unique_edges])\n",
    "\n",
    "# # Training Loop\n",
    "# for epoch in range(num_epochs):\n",
    "#     transe_model.train()\n",
    "#     optimizer.zero_grad()\n",
    "\n",
    "#     # Sample positive triplets\n",
    "#     idx = torch.randint(0, train_edges.shape[0], (batch_size,))\n",
    "#     pos_triplets = train_edges[idx]\n",
    "\n",
    "#     # Generate negative samples (corrupt the tail entity)\n",
    "#     neg_triplets = pos_triplets.clone()\n",
    "#     neg_triplets[:, 2] = torch.randint(0, num_entities, (batch_size,))\n",
    "\n",
    "#     # Compute scores\n",
    "#     pos_scores = transe_model.score(pos_triplets[:, 0], pos_triplets[:, 1], pos_triplets[:, 2])\n",
    "#     neg_scores = transe_model.score(neg_triplets[:, 0], neg_triplets[:, 1], neg_triplets[:, 2])\n",
    "\n",
    "#     # Compute loss (Ranking loss)\n",
    "#     loss = loss_fn(pos_scores, neg_scores, torch.ones_like(pos_scores))\n",
    "#     loss.backward()\n",
    "#     optimizer.step()\n",
    "\n",
    "#     # Print loss every 10 epochs\n",
    "#     if epoch % 10 == 0:\n",
    "#         print(f\"Epoch {epoch}: Loss = {loss.item():.4f}\")\n",
    "    \n",
    "\n",
    "# # Evaluation: Compute MRR and Hits@k\n",
    "# transe_model.eval()\n",
    "\n",
    "# ranks = []\n",
    "# for (head, relation, tail) in unique_edges:\n",
    "#     head_idx = torch.tensor([node_to_index[head]])\n",
    "#     relation_idx = torch.tensor([relation_to_index[relation]])\n",
    "#     tail_idx = torch.tensor([node_to_index[tail]])\n",
    "\n",
    "#     # Compute the score for the correct entity\n",
    "#     correct_score = transe_model.score(head_idx, relation_idx, tail_idx).item()\n",
    "\n",
    "#     # Compute scores for all possible tail entities\n",
    "#     all_tail_indices = torch.arange(num_entities)\n",
    "#     all_scores = transe_model.score(head_idx.repeat(num_entities), relation_idx.repeat(num_entities), all_tail_indices)\n",
    "\n",
    "#     # Rank the correct tail\n",
    "#     sorted_scores, sorted_indices = torch.sort(all_scores, descending=True)\n",
    "#     rank = (sorted_indices == tail_idx).nonzero(as_tuple=True)[0].item() + 1  # Convert 0-based to 1-based rank\n",
    "\n",
    "#     ranks.append(rank)\n",
    "\n",
    "# # Compute evaluation metrics\n",
    "# MR = np.mean(ranks)\n",
    "# MRR = np.mean([1.0 / r for r in ranks])\n",
    "# Hits_1 = np.mean([1 if r <= 1 else 0 for r in ranks])\n",
    "# Hits_3 = np.mean([1 if r <= 3 else 0 for r in ranks])\n",
    "# Hits_10 = np.mean([1 if r <= 10 else 0 for r in ranks])\n",
    "\n",
    "# # Save evaluation metrics\n",
    "# metrics_path = os.path.join(data_dir, \"transe_metrics.txt\")\n",
    "# with open(metrics_path, \"w\") as f:\n",
    "#     f.write(f\"Mean Rank (MR): {MR:.2f}\\n\")\n",
    "#     f.write(f\"Mean Reciprocal Rank (MRR): {MRR:.4f}\\n\")\n",
    "#     f.write(f\"Hits@1: {Hits_1:.4f}\\n\")\n",
    "#     f.write(f\"Hits@3: {Hits_3:.4f}\\n\")\n",
    "#     f.write(f\"Hits@10: {Hits_10:.4f}\\n\")\n",
    "\n",
    "# # Compute embeddings for all nodes\n",
    "# node_indices = torch.tensor([node_to_index[n] for n in unique_nodes])\n",
    "# node_embeddings = transe_model.entity_embeddings(node_indices).detach().numpy()\n",
    "\n",
    "# # Compute embeddings for all relations\n",
    "# relation_indices = torch.tensor([relation_to_index[r] for r in hetero_data.edge_types])\n",
    "# relation_embeddings = transe_model.relation_embeddings(relation_indices).detach().numpy()\n",
    "\n",
    "# # Compute embeddings for all edges\n",
    "# edge_embeddings = []\n",
    "# for (head, relation, tail) in unique_edges:\n",
    "#     head_idx = torch.tensor(node_to_index[head])\n",
    "#     relation_idx = torch.tensor(relation_to_index[relation])\n",
    "#     tail_idx = torch.tensor(node_to_index[tail])\n",
    "\n",
    "#     head_emb = transe_model.entity_embeddings(head_idx)\n",
    "#     relation_emb = transe_model.relation_embeddings(relation_idx)\n",
    "#     tail_emb = transe_model.entity_embeddings(tail_idx)\n",
    "\n",
    "#     # Compute edge embedding using TransE formulation\n",
    "#     edge_embedding = (head_emb + relation_emb - tail_emb).detach().numpy()\n",
    "#     edge_embeddings.append(edge_embedding)\n",
    "\n",
    "# # Convert to DataFrames\n",
    "# node_df = pd.DataFrame(node_embeddings, index=unique_nodes)\n",
    "# relation_df = pd.DataFrame(relation_embeddings, index=hetero_data.edge_types)\n",
    "# edge_df = pd.DataFrame(edge_embeddings, index=[f\"{h}-{r}-{t}\" for (h, r, t) in unique_edges])\n",
    "\n",
    "# # Save embeddings to files\n",
    "# node_embeddings_path = os.path.join(data_dir, \"node_embeddings.csv\")\n",
    "# relation_embeddings_path = os.path.join(data_dir, \"relation_embeddings.csv\")\n",
    "# edge_embeddings_path = os.path.join(data_dir, \"edge_embeddings.csv\")\n",
    "\n",
    "# node_df.to_csv(node_embeddings_path)\n",
    "# relation_df.to_csv(relation_embeddings_path)\n",
    "# edge_df.to_csv(edge_embeddings_path)\n",
    "\n",
    "# # Return saved file paths\n",
    "# node_embeddings_path, relation_embeddings_path, edge_embeddings_path, metrics_path\n",
    "\n",
    "\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "\n",
    "# Define TransE Model\n",
    "class TransE(nn.Module):\n",
    "    def __init__(self, num_entities, num_relations, embedding_dim):\n",
    "        super(TransE, self).__init__()\n",
    "        self.entity_embeddings = nn.Embedding(num_entities, embedding_dim)\n",
    "        self.relation_embeddings = nn.Embedding(num_relations, embedding_dim)\n",
    "        nn.init.xavier_uniform_(self.entity_embeddings.weight)\n",
    "        nn.init.xavier_uniform_(self.relation_embeddings.weight)\n",
    "\n",
    "    def forward(self, head, relation):\n",
    "        head_emb = self.entity_embeddings(head)\n",
    "        relation_emb = self.relation_embeddings(relation)\n",
    "        return head_emb + relation_emb\n",
    "\n",
    "    def score(self, head, relation, tail):\n",
    "        h_r = self.forward(head, relation)\n",
    "        t_emb = self.entity_embeddings(tail)\n",
    "        return -torch.norm(h_r - t_emb, p=2, dim=1)\n",
    "\n",
    "class TransETrainer:\n",
    "    def __init__(self, hetero_data, embedding_dim=128, num_epochs=100, batch_size=32, lr=0.01):\n",
    "        self.data_dir = \"./data\"\n",
    "        os.makedirs(self.data_dir, exist_ok=True)\n",
    "        self.node_embeddings_path = os.path.join(self.data_dir, \"node_embeddings.csv\")\n",
    "        self.relation_embeddings_path = os.path.join(self.data_dir, \"relation_embeddings.csv\")\n",
    "        self.edge_embeddings_path = os.path.join(self.data_dir, \"edge_embeddings.csv\")\n",
    "        self.metrics_path = os.path.join(self.data_dir, \"transe_metrics.txt\")\n",
    "        self.hetero_data = hetero_data\n",
    "        self.embedding_dim = embedding_dim\n",
    "        self.num_epochs = num_epochs\n",
    "        self.batch_size = batch_size\n",
    "        self.lr = lr\n",
    "        self.unique_nodes, self.unique_edges, self.node_to_index, self.relation_to_index = self._extract_graph_info()\n",
    "        self.num_entities = len(self.unique_nodes)\n",
    "        self.num_relations = len(self.hetero_data.edge_types)\n",
    "        self.transe_model = TransE(self.num_entities, self.num_relations, self.embedding_dim)\n",
    "        self.optimizer = optim.Adam(self.transe_model.parameters(), lr=self.lr)\n",
    "        self.loss_fn = nn.MarginRankingLoss(margin=1.0)\n",
    "    \n",
    "    def _extract_graph_info(self):\n",
    "        \"\"\"Extracts unique nodes, edges, and their mappings.\"\"\"\n",
    "        unique_nodes = set()\n",
    "        self.node_mapping = {}  # (node_type, local_index) -> global_index\n",
    "        self.global_to_node = {}  # global_index -> (node_type, local_index)\n",
    "        global_index = 0  \n",
    "\n",
    "        for node_type in self.hetero_data.node_types:\n",
    "            num_nodes = self.hetero_data[node_type].x.shape[0] if hasattr(self.hetero_data[node_type], \"x\") else 0\n",
    "            for local_idx in range(num_nodes):\n",
    "                self.node_mapping[(node_type, local_idx)] = global_index  # Forward mapping\n",
    "                self.global_to_node[global_index] = (node_type, local_idx)  # Reverse mapping\n",
    "                unique_nodes.add(global_index)\n",
    "                global_index += 1  # Increment global index\n",
    "\n",
    "        unique_nodes = sorted(list(unique_nodes))  # Ensure sorted list\n",
    "        node_to_index = {node: i for i, node in enumerate(unique_nodes)}\n",
    "\n",
    "        unique_edges = []\n",
    "        for edge_type in self.hetero_data.edge_types:\n",
    "            edge_index = self.hetero_data[edge_type].edge_index\n",
    "\n",
    "            for i in range(edge_index.shape[1]):\n",
    "                src_local = edge_index[0, i].item()\n",
    "                tgt_local = edge_index[1, i].item()\n",
    "\n",
    "                if (edge_type[0], src_local) in self.node_mapping and (edge_type[2], tgt_local) in self.node_mapping:\n",
    "                    src_global = self.node_mapping[(edge_type[0], src_local)]\n",
    "                    tgt_global = self.node_mapping[(edge_type[2], tgt_local)]\n",
    "                    unique_edges.append((src_global, edge_type, tgt_global))  # Store full tuple\n",
    "\n",
    "        relation_to_index = {rel: i for i, rel in enumerate(self.hetero_data.edge_types)}  # Store full tuple\n",
    "\n",
    "        return unique_nodes, unique_edges, node_to_index, relation_to_index\n",
    "\n",
    "    def get_embedding(self, global_index):\n",
    "        \"\"\"Retrieve TransE embedding and original node name for a given global index.\"\"\"\n",
    "        if global_index not in self.global_to_node:\n",
    "            raise ValueError(f\"Global index {global_index} not found in mapping.\")\n",
    "\n",
    "        node_type, local_index = self.global_to_node[global_index]\n",
    "        embedding = self.transe_model.entity_embeddings(torch.tensor([global_index])).detach().numpy()\n",
    "        \n",
    "        return embedding, f\"{node_type}{local_index}\"\n",
    "\n",
    "    def _save_embeddings(self):\n",
    "        \"\"\"Saves node, relation, and edge embeddings to CSV files.\"\"\"\n",
    "        print(\"Saving embeddings to disk...\")\n",
    "\n",
    "        node_indices = torch.arange(self.num_entities)\n",
    "        node_embeddings = self.transe_model.entity_embeddings(node_indices).detach().numpy()\n",
    "        node_df = pd.DataFrame(node_embeddings, index=self.unique_nodes)\n",
    "        node_df.to_csv(self.node_embeddings_path)\n",
    "\n",
    "        relation_indices = torch.arange(self.num_relations)\n",
    "        relation_embeddings = self.transe_model.relation_embeddings(relation_indices).detach().numpy()\n",
    "        relation_df = pd.DataFrame(relation_embeddings, index=self.hetero_data.edge_types)\n",
    "        relation_df.to_csv(self.relation_embeddings_path)\n",
    "\n",
    "        edge_embeddings = []\n",
    "        edge_index_list = []\n",
    "        for (head, relation, tail) in self.unique_edges:\n",
    "            head_idx = torch.tensor(self.node_to_index[head])\n",
    "            relation_idx = torch.tensor(self.relation_to_index[relation])\n",
    "            tail_idx = torch.tensor(self.node_to_index[tail])\n",
    "\n",
    "            head_emb = self.transe_model.entity_embeddings(head_idx)\n",
    "            relation_emb = self.transe_model.relation_embeddings(relation_idx)\n",
    "            tail_emb = self.transe_model.entity_embeddings(tail_idx)\n",
    "\n",
    "            edge_embedding = (head_emb + relation_emb - tail_emb).detach().numpy()\n",
    "            edge_embeddings.append(edge_embedding)\n",
    "            edge_index_list.append(f\"{head}-{relation}-{tail}\")\n",
    "\n",
    "        edge_df = pd.DataFrame(edge_embeddings, index=edge_index_list)\n",
    "        edge_df.to_csv(self.edge_embeddings_path)\n",
    "\n",
    "        print(f\"Embeddings saved:\\n - Nodes: {self.node_embeddings_path}\\n - Relations: {self.relation_embeddings_path}\\n - Edges: {self.edge_embeddings_path}\")\n",
    "\n",
    "    def _evaluate(self):\n",
    "        \"\"\"Computes evaluation metrics (MR, MRR, Hits@K).\"\"\"\n",
    "        self.transe_model.eval()\n",
    "        ranks = []\n",
    "\n",
    "        for (head, relation, tail) in self.unique_edges:\n",
    "            head_idx = torch.tensor([self.node_to_index[head]])\n",
    "            relation_idx = torch.tensor([self.relation_to_index[relation]])\n",
    "            tail_idx = torch.tensor([self.node_to_index[tail]])\n",
    "\n",
    "            correct_score = self.transe_model.score(head_idx, relation_idx, tail_idx).item()\n",
    "            all_tail_indices = torch.arange(self.num_entities)\n",
    "            all_scores = self.transe_model.score(head_idx.repeat(self.num_entities), relation_idx.repeat(self.num_entities), all_tail_indices)\n",
    "\n",
    "            sorted_scores, sorted_indices = torch.sort(all_scores, descending=True)\n",
    "            rank = (sorted_indices == tail_idx).nonzero(as_tuple=True)[0].item() + 1\n",
    "\n",
    "            ranks.append(rank)\n",
    "\n",
    "        MR = np.mean(ranks)\n",
    "        MRR = np.mean([1.0 / r for r in ranks])\n",
    "        Hits_1 = np.mean([1 if r <= 1 else 0 for r in ranks])\n",
    "        Hits_3 = np.mean([1 if r <= 3 else 0 for r in ranks])\n",
    "        Hits_10 = np.mean([1 if r <= 10 else 0 for r in ranks])\n",
    "\n",
    "        with open(self.metrics_path, \"w\") as f:\n",
    "            f.write(f\"Mean Rank (MR): {MR:.2f}\\n\")\n",
    "            f.write(f\"Mean Reciprocal Rank (MRR): {MRR:.4f}\\n\")\n",
    "            f.write(f\"Hits@1: {Hits_1:.4f}\\n\")\n",
    "            f.write(f\"Hits@3: {Hits_3:.4f}\\n\")\n",
    "            f.write(f\"Hits@10: {Hits_10:.4f}\\n\")\n",
    "\n",
    "        print(\"Evaluation complete.\")\n",
    "\n",
    "    def train(self):\n",
    "        if os.path.exists(self.node_embeddings_path):\n",
    "            print(\"Loaded TransE embeddings from file.\")\n",
    "            return\n",
    "        \n",
    "        print(\"TransE embeddings not found. Computing embeddings...\")\n",
    "        train_edges = torch.tensor([(self.node_to_index[h], self.relation_to_index[r], self.node_to_index[t]) for (h, r, t) in self.unique_edges])\n",
    "        \n",
    "        for epoch in range(self.num_epochs):\n",
    "            self.transe_model.train()\n",
    "            self.optimizer.zero_grad()\n",
    "            idx = torch.randint(0, train_edges.shape[0], (self.batch_size,))\n",
    "            pos_triplets = train_edges[idx]\n",
    "            neg_triplets = pos_triplets.clone()\n",
    "            neg_triplets[:, 2] = torch.randint(0, self.num_entities, (self.batch_size,))\n",
    "            pos_scores = self.transe_model.score(pos_triplets[:, 0], pos_triplets[:, 1], pos_triplets[:, 2])\n",
    "            neg_scores = self.transe_model.score(neg_triplets[:, 0], neg_triplets[:, 1], neg_triplets[:, 2])\n",
    "            loss = self.loss_fn(pos_scores, neg_scores, torch.ones_like(pos_scores))\n",
    "            loss.backward()\n",
    "            self.optimizer.step()\n",
    "\n",
    "        self._save_embeddings()\n",
    "        self._evaluate()\n",
    "\n",
    "# Initialize and train TransE model\n",
    "transe_trainer = TransETrainer(hetero_data)\n",
    "transe_trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "3f0a40b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "import functools\n",
    "\n",
    "import numpy as np\n",
    "from owlapy.class_expression import OWLClassExpression, OWLObjectComplementOf, OWLObjectUnionOf, \\\n",
    "    OWLObjectIntersectionOf, OWLObjectSomeValuesFrom, OWLObjectAllValuesFrom, OWLObjectMaxCardinality, \\\n",
    "    OWLObjectMinCardinality, OWLClass, OWLDataSomeValuesFrom, OWLObjectOneOf\n",
    "from owlapy.owl_property import OWLObjectProperty\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "\n",
    "class Evaluator:\n",
    "    \"\"\" An evaluator which is able to evaluate the accuracy of a given logical formula based on a given dataset.\"\"\"\n",
    "\n",
    "    def __init__(self, data: HeteroData, labeled_nodeset: set[tuple[int, str]] = set()):\n",
    "        \"\"\"\"\n",
    "        Initializes the evaluator based on the given dataset. After the initialization the object should be able to\n",
    "        evaluate logical formulas based on the dataset.\n",
    "\n",
    "        Args:\n",
    "            data: The dataset which should be used for evaluation.\n",
    "        \"\"\"\"\"\n",
    "        self._data = data\n",
    "        self._nodeset = self._get_nodeset()\n",
    "        self._labeled_nodeset = labeled_nodeset\n",
    "\n",
    "        self.owl_mapping = {\n",
    "            OWLObjectComplementOf: self._eval_complement,\n",
    "            OWLObjectUnionOf: self._eval_union,\n",
    "            OWLObjectIntersectionOf: self._eval_intersection,\n",
    "            OWLObjectSomeValuesFrom: self._eval_existential,\n",
    "            OWLObjectAllValuesFrom: self._eval_universal,\n",
    "            OWLObjectMaxCardinality: self._eval_max_cardinality,\n",
    "            OWLObjectMinCardinality: self._eval_min_cardinality,\n",
    "            OWLClass: self._eval_class,\n",
    "            OWLDataSomeValuesFrom: self._eval_property_value,\n",
    "            OWLObjectOneOf: self._eval_object_one_of\n",
    "        }\n",
    "\n",
    "    @property\n",
    "    def data(self) -> HeteroData:\n",
    "        \"\"\"\n",
    "        The dataset which should be used for evaluation.\n",
    "\n",
    "        Returns:\n",
    "            The dataset which should be used for evaluation.\n",
    "        \"\"\"\n",
    "        return self._data\n",
    "\n",
    "    @data.setter\n",
    "    def data(self, val: HeteroData) -> None:\n",
    "        \"\"\"\n",
    "        Sets the dataset which should be used for evaluation to the given value.\n",
    "\n",
    "        Args:\n",
    "            val: The dataset which should be used for evaluation.\n",
    "        \"\"\"\n",
    "        self._data = val\n",
    "\n",
    "    def explanation_accuracy(self, ground_truth: set[tuple[int, str]],\n",
    "                             logical_formula: OWLClassExpression) -> tuple[float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculates the explanation accuracy of the given logical formula based on the given ground truth.\n",
    "\n",
    "        Args:\n",
    "            ground_truth: The ground truth which should be used for evaluation.\n",
    "            logical_formula: The logical formula which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A triple containing the precision, the recall and the accuracy of the given logical formula based on the given\n",
    "            ground truth.\n",
    "        \"\"\"\n",
    "        tp, fp, tn, fn = self._get_positive_negatives(ground_truth, logical_formula)\n",
    "        precision = tp / (tp + fp) if tp + fp > 0 else 0\n",
    "        recall = tp / (tp + fn) if tp + fn > 0 else 0\n",
    "        accuracy = (tp + tn) / (tp + fp + tn + fn)\n",
    "        return precision, recall, accuracy\n",
    "\n",
    "    def f1_score(self, ground_truth: set[tuple[int, str]], logical_formula: OWLClassExpression) -> float:\n",
    "        \"\"\"\n",
    "        Calculates the F1 score of the given logical formula based on the given ground truth.\n",
    "        Args:\n",
    "            ground_truth: The ground truth which should be used for evaluation.\n",
    "            logical_formula: The logical formula which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            The F1 score of the given logical formula based on the given ground truth.\n",
    "        \"\"\"\n",
    "        tp, fp, _, fn = self._get_positive_negatives(ground_truth, logical_formula)\n",
    "\n",
    "        return (2 * tp) / (2 * tp + fp + fn)\n",
    "\n",
    "    def _get_positive_negatives(self, ground_truth: set[tuple[int, str]], logical_formula: OWLClassExpression) \\\n",
    "            -> tuple[float, float, float, float]:\n",
    "        \"\"\"\n",
    "        Calculates the sizes of the true positives, false positives, false negatives and true negatives of the given\n",
    "        logical formula.\n",
    "        Args:\n",
    "            ground_truth: The ground truth which should be used for evaluation.\n",
    "            logical_formula: The logical formula which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A tuple containing the sizes of the true positives, false positives, true negatives and false negatives of\n",
    "            the given logical formula.\n",
    "        \"\"\"\n",
    "        explanation_set = self._eval_formula(logical_formula) & self._labeled_nodeset # we need to filter out every node that is not in the test set, or we overestimate the false positives\n",
    "        true_positives = len(explanation_set & ground_truth)\n",
    "        false_positives = len(explanation_set - ground_truth)\n",
    "        false_negatives = len(ground_truth - explanation_set)\n",
    "        true_negatives = len(self._labeled_nodeset) - true_positives - false_positives - false_negatives # replace self.data.num_nodes with the size of the test set\n",
    "\n",
    "        return true_positives, false_positives, true_negatives, false_negatives\n",
    "\n",
    "    #@functools.lru_cache(maxsize=100)\n",
    "    def _eval_formula(self, logical_formula: OWLClassExpression) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given logical formula based on the given dataset and returns the set of matching nodes.\n",
    "\n",
    "        Args:\n",
    "            logical_formula: The logical formula which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes which are the result of the evaluation.\n",
    "        \"\"\"\n",
    "        return self.owl_mapping[type(logical_formula)](logical_formula)\n",
    "\n",
    "    def _eval_complement(self, logical_formula: OWLObjectComplementOf) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given complement based on the given dataset and returns the set of matching nodes.\n",
    "        which are the complement of the inner set.\n",
    "\n",
    "        Args:\n",
    "            logical_formula: The complement which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes which are the result of the evaluation.\n",
    "        \"\"\"\n",
    "        inner_set = self._eval_formula(logical_formula.get_operand())\n",
    "        return self._nodeset - inner_set\n",
    "\n",
    "    def _eval_union(self, logical_formula: OWLObjectUnionOf) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given union based on the given dataset and returns the set of matching nodes.\n",
    "        Args:\n",
    "            logical_formula: The union which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes which are the result of the evaluation.\n",
    "        \"\"\"\n",
    "        operands = list(logical_formula.operands())\n",
    "        result = set()\n",
    "        for i in operands:\n",
    "            result = result | self._eval_formula(i)\n",
    "        return result\n",
    "\n",
    "    def _eval_intersection(self, logical_formula: OWLObjectIntersectionOf) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given intersection based on the given dataset and returns the set of matching nodes.\n",
    "        Args:\n",
    "            logical_formula: The intersection which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes which are the result of the evaluation.\n",
    "        \"\"\"\n",
    "        operands = list(logical_formula.operands())\n",
    "        result = self._eval_formula(operands[0])\n",
    "        for i in operands[1:]:\n",
    "            result = result & self._eval_formula(i)\n",
    "        return result\n",
    "\n",
    "    def _eval_existential(self, logical_formula: OWLObjectSomeValuesFrom) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given existential based on the given dataset and returns the set of matching nodes.\n",
    "        Args:\n",
    "            logical_formula: The existential restriction which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes which are the result of the evaluation.\n",
    "        \"\"\"\n",
    "        dest = self._eval_formula(logical_formula.get_filler())\n",
    "        edge_type = self._eval_property(logical_formula.get_property())\n",
    "        dest_first_elements = np.array([b[0] for b in dest])\n",
    "        selection = np.isin(self.data[edge_type]['edge_index'][1].cpu(), dest_first_elements)\n",
    "        origin = self.data[edge_type]['edge_index'][0][selection].cpu().numpy()\n",
    "        return set(zip(origin, [edge_type[0], ] * len(origin)))\n",
    "\n",
    "    def _eval_object_one_of(self, logical_formula: OWLObjectOneOf) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluate an OWL ObjectOneOf logical formula and return a set of tuples\n",
    "        representing nodes that match the condition.\n",
    "\n",
    "        Args:\n",
    "            logical_formula: The OWL ObjectOneOf logical formula to evaluate.\n",
    "\n",
    "        Returns:\n",
    "            A set of tuples where each tuple represents a node that matches the condition.\n",
    "            Each tuple contains two elements: an integer representing the index and a string representing the node type.\n",
    "        \"\"\"\n",
    "        nodes = set()\n",
    "        individuals = list(logical_formula.individuals())\n",
    "        for individual in individuals:\n",
    "            node_type, index = individual.get_iri().get_remainder().split('#')\n",
    "            nodes.add((int(index), node_type))\n",
    "        return nodes\n",
    "\n",
    "    def _eval_universal(self, logical_formula: OWLObjectAllValuesFrom) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given universal based on the given dataset and returns the set of matching nodes.\n",
    "        Args:\n",
    "            logical_formula: The universal restriction which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes which are the result of the evaluation.\n",
    "        \"\"\"\n",
    "        dest = set(self._eval_formula(logical_formula.get_filler()))\n",
    "        edge_type = self._eval_property(logical_formula.get_property())\n",
    "        result = set()\n",
    "\n",
    "        mapping = dict()\n",
    "\n",
    "        # Convert edge_index arrays to NumPy arrays for better performance\n",
    "        edge_index_0 = self.data[edge_type][\"edge_index\"][0].cpu().numpy()\n",
    "        edge_index_1 = self.data[edge_type][\"edge_index\"][1].cpu().numpy()\n",
    "\n",
    "        for i in range(len(edge_index_0)):\n",
    "            idx_0 = edge_index_0[i].item()\n",
    "            idx_1 = edge_index_1[i].item()\n",
    "\n",
    "            if idx_0 not in mapping:\n",
    "                mapping[idx_0] = [idx_1]\n",
    "            else:\n",
    "                mapping[idx_0].append(idx_1)\n",
    "\n",
    "        for i, indices in mapping.items():\n",
    "            check_set = {(idx, edge_type[2]) for idx in indices}\n",
    "            if check_set.issubset(dest):\n",
    "                result.add((i, edge_type[0]))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _eval_max_cardinality(self, logical_formula: OWLObjectMaxCardinality) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given max cardinality restriction based on the given dataset and returns the set of matching\n",
    "        nodes.\n",
    "\n",
    "        Args:\n",
    "            logical_formula: The max cardinality restriction which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes which are the result of the evaluation.\n",
    "        \"\"\"\n",
    "        dest = set(self._eval_formula(logical_formula.get_filler()))\n",
    "        edge_type = self._eval_property(logical_formula.get_property())\n",
    "        cardinality = logical_formula.get_cardinality()\n",
    "        result = set()\n",
    "\n",
    "        mapping = dict()\n",
    "\n",
    "        # Convert edge_index arrays to NumPy arrays for better performance\n",
    "        edge_index_0 = self.data[edge_type][\"edge_index\"][0].cpu().numpy()\n",
    "        edge_index_1 = self.data[edge_type][\"edge_index\"][1].cpu().numpy()\n",
    "\n",
    "        for i in range(len(edge_index_0)):\n",
    "            idx_0 = edge_index_0[i].item()\n",
    "            idx_1 = edge_index_1[i].item()\n",
    "\n",
    "            if idx_0 not in mapping:\n",
    "                mapping[idx_0] = [idx_1]\n",
    "            else:\n",
    "                mapping[idx_0].append(idx_1)\n",
    "\n",
    "        for i, indices in mapping.items():\n",
    "            check_set = {(idx, edge_type[2]) for idx in indices}\n",
    "            if len(check_set) <= cardinality and check_set.issubset(dest):\n",
    "                result.add((i, edge_type[0]))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _eval_min_cardinality(self, logical_formula: OWLObjectMinCardinality) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given min cardinality restriction based on the given dataset and returns the\n",
    "        set of matching nodes.\n",
    "\n",
    "        Args:\n",
    "            logical_formula: The min cardinality restriction which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes which are the result of the evaluation.\n",
    "        \"\"\"\n",
    "        dest = set(self._eval_formula(logical_formula.get_filler()))\n",
    "        edge_type = self._eval_property(logical_formula.get_property())\n",
    "        cardinality = logical_formula.get_cardinality()\n",
    "        result = set()\n",
    "\n",
    "        mapping = dict()\n",
    "\n",
    "        # Convert edge_index arrays to NumPy arrays for better performance\n",
    "        edge_index_0 = self.data[edge_type][\"edge_index\"][0].cpu().numpy()\n",
    "        edge_index_1 = self.data[edge_type][\"edge_index\"][1].cpu().numpy()\n",
    "\n",
    "        for i in range(len(edge_index_0)):\n",
    "            idx_0 = edge_index_0[i].item()\n",
    "            idx_1 = edge_index_1[i].item()\n",
    "\n",
    "            if idx_0 not in mapping:\n",
    "                mapping[idx_0] = [idx_1]\n",
    "            else:\n",
    "                mapping[idx_0].append(idx_1)\n",
    "\n",
    "        for i, indices in mapping.items():\n",
    "            check_set = {(idx, edge_type[2]) for idx in indices}\n",
    "            if len(check_set) >= cardinality and check_set.issubset(dest):\n",
    "                result.add((i, edge_type[0]))\n",
    "\n",
    "        return result\n",
    "\n",
    "    def _eval_class(self, logical_formula: OWLClass) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given class based on the given dataset and returns the set of matching nodes.\n",
    "        Args:\n",
    "            logical_formula: The class which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes which are the result of the evaluation.\n",
    "        \"\"\"\n",
    "        return self._get_nodeset([logical_formula.get_iri().get_remainder(), ]) # mask with train/test mask\n",
    "\n",
    "    def _eval_property_value(self, logical_formula: OWLDataSomeValuesFrom) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Evaluates the given OWLDataSomeValuesFrom logical formula based on the dataset and returns the set of nodes\n",
    "        that satisfy the specified property value condition.\n",
    "\n",
    "        Args:\n",
    "            logical_formula: The OWLDataSomeValuesFrom expression representing a property value condition.\n",
    "\n",
    "        Returns:\n",
    "            A set of nodes that satisfy the specified property value condition.\n",
    "                                 Each tuple contains the node index and node type.\n",
    "        \"\"\"\n",
    "        nodes_matching_condition = set()\n",
    "\n",
    "        # Extract information from the logical formula\n",
    "        property_iri = logical_formula.get_property().get_iri().get_remainder()\n",
    "        facet_restriction = logical_formula.get_filler().get_facet_restrictions()[0]\n",
    "\n",
    "        # Parse property information\n",
    "        property_split = property_iri.split('_')\n",
    "        node_type = property_split[0]\n",
    "        feature_index = int(property_split[-1]) - 1\n",
    "\n",
    "        # Extract operator and comparison value from facet restriction\n",
    "        operator = facet_restriction.get_facet().operator\n",
    "        comparison_value = facet_restriction.get_facet_value()._v\n",
    "\n",
    "        # Retrieve nodes and evaluate the condition\n",
    "        nodes = self.data[node_type]['x'].cpu().numpy()\n",
    "        for index, node in enumerate(nodes):\n",
    "            if operator(node[feature_index], comparison_value):\n",
    "                nodes_matching_condition.add((index, node_type))\n",
    "\n",
    "        return nodes_matching_condition\n",
    "\n",
    "    def _eval_property(self, property: OWLObjectProperty) -> tuple[str, str, str]:\n",
    "        \"\"\"\n",
    "        Evaluates the given property based on the given dataset and returns the edge type.\n",
    "        Args:\n",
    "            property: The property which should be evaluated.\n",
    "\n",
    "        Returns:\n",
    "            The edge type which is the result of the evaluation.\n",
    "        \"\"\"\n",
    "        for i in self.data.edge_types:\n",
    "            if i[1] == property.get_iri().get_remainder():\n",
    "                return i\n",
    "\n",
    "    def _get_nodeset(self, node_types: list[str] = None) -> set[tuple[int, str]]:\n",
    "        \"\"\"\n",
    "        Returns the set of nodes of the given node types.\n",
    "        Args:\n",
    "            node_types: The node types for which the nodes should be returned.\n",
    "\n",
    "        Returns:\n",
    "            The set of nodes of the given node types.\n",
    "        \"\"\"\n",
    "        if node_types is None or node_types == ['Thing', ]:\n",
    "            node_types = self.data.node_types\n",
    "        if node_types == ['Nothing', ]:\n",
    "            return set()\n",
    "        result = set()\n",
    "        for i in node_types:\n",
    "            result = result | set(enumerate([i] * self.data[i][\"x\"].shape[0]))\n",
    "        return result\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "id": "ffac3215",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded TransE embeddings from file.\n",
      " Loaded GNN model from ./saved_models/A_gnn.pth\n",
      "Starting RL with positive nodes: [0, 1, 2]\n",
      "Episode 1/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 2/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 3/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 4/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 5/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 6/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 7/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 8/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 9/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 10/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 11/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 12/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 13/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 14/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 15/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 16/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 17/100: Path Taken = [np.int64(0), np.int64(5)], Reward = -0.13333333333333333\n",
      "Episode 18/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 19/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 20/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 21/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 22/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 23/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 24/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 25/100: Path Taken = [np.int64(0), np.int64(5)], Reward = -0.13333333333333333\n",
      "Episode 26/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 27/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 28/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 29/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 30/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 31/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 32/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 33/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 34/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 35/100: Path Taken = [np.int64(0), np.int64(5)], Reward = -0.13333333333333333\n",
      "Episode 36/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 37/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 38/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 39/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 40/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 41/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 42/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 43/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 44/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 45/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 46/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 47/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 48/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 49/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 50/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 51/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 52/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 53/100: Path Taken = [np.int64(0), np.int64(5)], Reward = -0.13333333333333333\n",
      "Episode 54/100: Path Taken = [np.int64(0), np.int64(5)], Reward = -0.13333333333333333\n",
      "Episode 55/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 56/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 57/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 58/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 59/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 60/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 61/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 62/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 63/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 64/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 65/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 66/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 67/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 68/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 69/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 70/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 71/100: Path Taken = [np.int64(0), np.int64(5)], Reward = -0.13333333333333333\n",
      "Episode 72/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 73/100: Path Taken = [np.int64(0), np.int64(5)], Reward = -0.13333333333333333\n",
      "Episode 74/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 75/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 76/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 77/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 78/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 79/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 80/100: Path Taken = [np.int64(0), np.int64(5)], Reward = -0.13333333333333333\n",
      "Episode 81/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 82/100: Path Taken = [np.int64(2), np.int64(12)], Reward = -0.13333333333333333\n",
      "Episode 83/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 84/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 85/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 86/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 87/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 88/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 89/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 90/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 91/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 92/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 93/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 94/100: Path Taken = [np.int64(2), np.int64(7)], Reward = -0.13333333333333333\n",
      "Episode 95/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 96/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "Episode 97/100: Path Taken = [np.int64(0), np.int64(5)], Reward = -0.13333333333333333\n",
      "Episode 98/100: Path Taken = [np.int64(1), np.int64(6)], Reward = -0.13333333333333333\n",
      "Episode 99/100: Path Taken = [np.int64(1), np.int64(11)], Reward = -0.13333333333333333\n",
      "Episode 100/100: Path Taken = [np.int64(0), np.int64(10)], Reward = -0.13333333333333333\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import numpy as np\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "# ==================== RL Graph Environment ====================\n",
    "class RLGraphEnv:\n",
    "    def __init__(self, hetero_data, trainer):\n",
    "        self.hetero_data = hetero_data\n",
    "        self.trainer = trainer\n",
    "        self.current_node = None\n",
    "        self.path = []\n",
    "        self.tracked_paths = []\n",
    "\n",
    "    def reset(self, start_node):\n",
    "        self.current_node = start_node\n",
    "        self.path = [start_node]\n",
    "        return self.current_node\n",
    "\n",
    "    def step(self, action):\n",
    "        next_node = action\n",
    "        self.path.append(next_node)\n",
    "        self.current_node = next_node\n",
    "        done = len(self.path) >= 2\n",
    "        if done:\n",
    "            self.tracked_paths.append(self.path.copy())\n",
    "        return next_node, done\n",
    "\n",
    "    def get_neighbors(self, node):\n",
    "        neighbors = set()\n",
    "        for (src, _, tgt) in self.trainer.unique_edges:\n",
    "            if src == node:\n",
    "                neighbors.add(tgt)\n",
    "        return list(neighbors) if neighbors else list(self.trainer.global_to_node.keys())\n",
    "\n",
    "    def get_node_embedding(self, node):\n",
    "        return self.trainer.get_embedding(node)\n",
    "\n",
    "# ==================== Policy Network (REINFORCE) ====================\n",
    "class PolicyNetwork(nn.Module):\n",
    "    def __init__(self, state_dim, action_space, learning_rate=0.001):\n",
    "        super(PolicyNetwork, self).__init__()\n",
    "        self.fc1 = nn.Linear(state_dim, 128)\n",
    "        self.fc2 = nn.Linear(128, action_space)\n",
    "        self.optimizer = optim.Adam(self.parameters(), lr=learning_rate)\n",
    "\n",
    "    def forward(self, state):\n",
    "        x = torch.relu(self.fc1(state))\n",
    "        return torch.softmax(self.fc2(x), dim=-1).squeeze(0)  # Remove batch dim\n",
    "\n",
    "\n",
    "    def update(self, state_batch, action_batch, rewards):\n",
    "        action_probs = self.forward(state_batch)\n",
    "\n",
    "        # Ensure action_probs is at least 2D\n",
    "        if action_probs.dim() == 1:\n",
    "            action_probs = action_probs.unsqueeze(0)\n",
    "\n",
    "        # Ensure action_batch is (batch_size, 1)\n",
    "        action_batch = action_batch.view(-1, 1)\n",
    "\n",
    "        # # Debugging Prints\n",
    "        # print(f\"action_probs shape: {action_probs.shape}\")  # (batch_size, action_space)\n",
    "        # print(f\"action_batch shape: {action_batch.shape}\")  # (batch_size, 1)\n",
    "        # print(f\"action_probs: {action_probs}\")  \n",
    "        # print(f\"action_batch: {action_batch}\")  \n",
    "\n",
    "        # Validate indices before gathering\n",
    "        max_index = action_probs.shape[1] - 1\n",
    "        action_batch = torch.clamp(action_batch, 0, max_index)\n",
    "\n",
    "        # Apply gather and compute log probability\n",
    "        action_log_probs = torch.log(action_probs.gather(1, action_batch).squeeze())\n",
    "\n",
    "        loss = -torch.sum(action_log_probs * rewards)\n",
    "\n",
    "        self.optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        self.optimizer.step()\n",
    "\n",
    "# ==================== Logical Expression Generator ====================\n",
    "def paths_to_logical_expressions(paths, node_mapping):\n",
    "    logical_expressions = []\n",
    "    for path in paths:\n",
    "        if len(path) < 2:\n",
    "            continue\n",
    "        owl_classes = [f\"∃ connectedTo.{node_mapping[n][0]}{node_mapping[n][1]}\" for n in path if n in node_mapping]\n",
    "        logical_expressions.append(\" AND \".join(owl_classes))\n",
    "    return logical_expressions\n",
    "\n",
    "# ==================== Train RL Agent ====================\n",
    "def train_rl_agent_with_owl(hetero_data, num_episodes=100):\n",
    "    trainer = TransETrainer(hetero_data)\n",
    "    trainer.train()\n",
    "\n",
    "    env = RLGraphEnv(hetero_data, trainer)\n",
    "\n",
    "    state_dim = trainer.transe_model.entity_embeddings.weight.shape[1]\n",
    "    policy_net = PolicyNetwork(state_dim, len(trainer.node_mapping))\n",
    "\n",
    "    optimizer = optim.Adam(policy_net.parameters(), lr=0.01)\n",
    "\n",
    "    gnn_trainer = GNNTrainer(hetero_data, node_type='A')\n",
    "    gnn_trainer.train()\n",
    "    positive_nodes = gnn_trainer.get_positive_nodes()\n",
    "\n",
    "    if not positive_nodes:\n",
    "        positive_nodes = list(trainer.global_to_node.keys())\n",
    "\n",
    "    print(\"Starting RL with positive nodes:\", positive_nodes)\n",
    "\n",
    "    for episode in range(num_episodes):\n",
    "        start_node = np.random.choice(positive_nodes)\n",
    "        state = env.reset(start_node)\n",
    "\n",
    "        episode_states, episode_actions, rewards = [], [], []\n",
    "\n",
    "        for _ in range(2):\n",
    "            neighbors = env.get_neighbors(state)\n",
    "\n",
    "            if not neighbors:\n",
    "                action = np.random.choice(list(trainer.global_to_node.keys()))\n",
    "            else:\n",
    "                state_emb, _ = env.get_node_embedding(state)\n",
    "                state_emb = torch.tensor(state_emb, dtype=torch.float32)\n",
    "\n",
    "                action_probs = policy_net.forward(state_emb).detach().numpy().flatten()\n",
    "\n",
    "                # Debugging Prints\n",
    "                # print(f\"State {state}: Action probabilities shape: {action_probs.shape}\")\n",
    "\n",
    "                neighbor_indices = [n for n in neighbors if 0 <= n < len(action_probs)]\n",
    "                # print(f\"State {state}: Filtered neighbor indices: {neighbor_indices}\")\n",
    "\n",
    "                if neighbor_indices:\n",
    "                    valid_probs = action_probs[neighbor_indices]\n",
    "\n",
    "                    if valid_probs.sum() > 0:\n",
    "                        valid_probs /= valid_probs.sum()\n",
    "                        action = np.random.choice(neighbor_indices, p=valid_probs)\n",
    "                    else:\n",
    "                        action = np.random.choice(neighbor_indices)\n",
    "                else:\n",
    "                    action = np.random.choice(neighbors)\n",
    "\n",
    "            episode_states.append(state_emb)\n",
    "            episode_actions.append(action)\n",
    "\n",
    "            state, done = env.step(action)\n",
    "            if done:\n",
    "                break\n",
    "\n",
    "        if episode_states:\n",
    "            action_batch = torch.tensor(episode_actions, dtype=torch.long)\n",
    "            state_batch = torch.stack(episode_states)\n",
    "\n",
    "            retrieved_nodes = set(env.path)\n",
    "            reward = (len(set(trainer.node_mapping.keys()) & retrieved_nodes) - \n",
    "                      len(retrieved_nodes - set(trainer.node_mapping.keys()))) / len(trainer.node_mapping)\n",
    "            rewards.append(reward)\n",
    "\n",
    "            returns = torch.tensor([sum(rewards[i:]) for i in range(len(rewards))], dtype=torch.float32)\n",
    "            policy_net.update(state_batch, action_batch, returns)\n",
    "\n",
    "        print(f\"Episode {episode + 1}/{num_episodes}: Path Taken = {env.path}, Reward = {reward}\")\n",
    "\n",
    "    logical_expressions = paths_to_logical_expressions(env.tracked_paths, trainer.node_mapping)\n",
    "    return logical_expressions\n",
    "\n",
    "# ==================== Run RL Training ====================\n",
    "logical_expressions = train_rl_agent_with_owl(hetero_data)\n",
    "\n",
    "for expr in logical_expressions:\n",
    "    print(expr)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f930380",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
